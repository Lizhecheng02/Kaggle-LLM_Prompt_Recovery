import pandas as pd
from tqdm import tqdm
import asyncio
import aiohttp
import os
import json
from dotenv import find_dotenv, load_dotenv
import logging
_ = load_dotenv(find_dotenv())


async def get_response_async(msg, model="gpt-4-1106-preview"):
    url = 'http://gpt-proxy.jd.com/gateway/azure/chat/completions'
    payload = json.dumps({
        "messages": [
            {
                "role": "user",
                "content": msg
            }
        ],
        "stream": False,
        "model": model,
        "temperature": 0.5,
        "presence_penalty": 0,
        "frequency_penalty": 0,
        "max_tokens": 2000
    })
    headers = {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer ' + os.environ["OPENAI_API_KEY"]
    }

    async with aiohttp.ClientSession() as session:
        for _ in range(5):
            async with session.post(url, headers=headers, data=payload) as response:
                if response.status == 200:
                    text = await response.text()
                    res = json.loads(text)
                    try:
                        return res['choices'][0]['message']['content']
                    except KeyError:
                        if _ != 4:
                            print(f"Request failed, retrying... {res}")
                            continue
                        else:
                            return res
                else:
                    print(
                        f"Request failed with status {response.status}, retrying...")
    return None


def formatting_prompts_func(examples):
    inst = """
You'll be given an original text and a rewritten text generated by an LLM. The prompt that guided the LLM's changes will be given as well. 
Suppose you don't know the original prompt, your task is to analyze the differences between an original text and a rewritten version generated by an LLM and try to infer it based on the differences you see. 
Provide a detailed explanation of how you arrived at your inference step by step.

**Original Text**:
{}

**Prompt**:
{}

**Rewritten Text**
{}

You should response in the following format:
**Inferred Promp**: ...

**Chain of Thoughts**: ...
"""
    return inst.format(examples['original_text'], examples['rewrite_prompt'], examples['rewritten_text'])





async def main():
    df = pd.read_csv('../kaggle_dataset/archive/lb_related_5k.csv')

    semaphore = asyncio.Semaphore(20)
    async def process_row_with_semaphore(row, func):
        async with semaphore:
            return await func(row)
    cot_task = [process_row_with_semaphore(formatting_prompts_func(
        row), get_response_async) for _, row in df.iterrows()]
    df['cot'] = await asyncio.gather(*cot_task)
    df.to_csv('./datasets_with_cot.csv', index = False)
    logging.info("Done")

asyncio.run(main())

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0559d487-07a5-4cc8-89f9-6e9386aaf869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87a239-3348-41d7-b7db-e3a06c092dde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_seq_length = 4096  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "# Use 4bit quantization to reduce memory usage. Can be False.\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb70b1-5ddb-48ee-8384-4af6eb96cff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./datasets/gemini_generated/gemini_dataset_v0.csv')\n",
    "df2 = pd.read_csv('./datasets/gemma10000.csv')\n",
    "df = pd.concat([df, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3490f-4f86-4152-84e5-bfaea5cbffdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767a3fbd-2973-4979-9d1e-80de48504858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/datasets_with_cot(1).csv')\n",
    "df = df.query('cot == cot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb119bda-78ab-452f-aeee-af1f3ddb2142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remove_ids = []\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        row['rewritten_text'].split('\\n\\n')[1]\n",
    "    except:\n",
    "        remove_ids.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae2c9ca-d096-4573-ba52-d4a5bea1a7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop(remove_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d27085-c0bb-41bc-92b6-452ae807d925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['rewritten_text'] = df['rewritten_text'].apply(lambda x: x.split('\\n\\n')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa08d67-302a-4a44-bf48-8f1f10505e67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9ab24-e70d-4507-b738-f3d313bc9f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-it-bnb-4bit\",  # Instruct version of Gemma 7b\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-it-bnb-4bit\",  # Instruct version of Gemma 2b\n",
    "]  # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"./gemma/7b-it/\",  # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc3cbe-d4d9-4ba3-9c55-902cb1edfd0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=True,\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a3b5b-b227-4ea7-aa44-d97c063c20fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df1 = pd.read_csv('./datasets/gemma10000.csv')\n",
    "# df2 = pd.read_csv('./plots2k.csv')\n",
    "# df2 = df2.drop(1745).reset_index(drop = True)\n",
    "# df2.columns = ['original_text','rewritten_text','rewrite_prompt']\n",
    "# df1 = df1[df2.columns]\n",
    "# df = pd.concat([df1, df2]).sample(frac = 1).reset_index(drop = True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfc976-3a33-4ec7-9f40-fe8ebf2d4d17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def formatting_prompts_func(examples):\n",
    "#     USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}<end_of_turn>\"\n",
    "#     inst = \"\"\"You will be given original text and rewritten text.\n",
    "# Analyzing the changes in style, theme, etc., your task is to recovery the prompt which LLM used to convert the original text.\n",
    "# **Original Text**:\n",
    "# {}\n",
    "# **Rewritten Text**\n",
    "# {}\n",
    "# \"\"\"\n",
    "#     original_texts = examples[\"original_text\"]\n",
    "#     rewrite_prompts       = examples[\"rewrite_prompt\"]\n",
    "#     rewritten_texts      = examples[\"rewritten_text\"]\n",
    "#     texts = []\n",
    "#     EOS_TOKEN = tokenizer.eos_token\n",
    "#     for original_text, rewritten_text, rewrite_prompt in zip(original_texts, rewritten_texts, rewrite_prompts):\n",
    "#         # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "#         inst = inst.format(original_text[:750],rewritten_text[:750])\n",
    "#         text = USER_CHAT_TEMPLATE.format(inst, rewrite_prompt) + EOS_TOKEN\n",
    "#         texts.append(text)\n",
    "#     return { \"text\" : texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2adb1-7ec8-4c54-95e0-218362ad1cad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "prompts = list(df['rewrite_prompt'].unique())\n",
    "train_prompts, test_prompts = train_test_split(\n",
    "    prompts, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b151a10-d2e5-4b78-8566-24a73ac63639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = df[df['rewrite_prompt'].isin(train_prompts)]\n",
    "test_df = df[~df['rewrite_prompt'].isin(train_prompts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6e3ea-bc1a-4824-af6f-f2b98b8d0c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_data = Dataset.from_pandas(train_df)\n",
    "test_data = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60ed16b-5814-4730-86a9-23c3b9eccb45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# ds = Dataset.from_pandas(df)\n",
    "# ds_split = ds.train_test_split(test_size=0.1, seed = 777)\n",
    "# train_data = ds_split[\"train\"]\n",
    "# test_data = ds_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1943ce-ead8-4ca3-94a7-1dd0735467b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def truncate_txt(text, length):\n",
    "\n",
    "    text_list = text.split()\n",
    "\n",
    "\n",
    "    if len(text_list) <= length:\n",
    "        return text\n",
    "\n",
    "\n",
    "    return \" \".join(text_list[:length])\n",
    "\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "\n",
    "    USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{}<end_of_turn>\\n<start_of_turn>model\\n{}<end_of_turn>\"\n",
    "\n",
    "#         inst = \"\"\"You'll be given an original text and a rewritten text generated by an LLM.\n",
    "\n",
    "# Analyze the stylistic changes in the rewritten version and identify the likely prompt that led to those changes.\n",
    "\n",
    "# Notice only output the prompt.\n",
    "\n",
    "# Here's what to focus on:\n",
    "\n",
    "# Shifts in Style: Look for changes in:\n",
    "\n",
    "# -Genre: (sci-fi, fantasy, historical fiction, etc.)\n",
    "\n",
    "# -Tone: (serious, humorous, conversational, etc.)\n",
    "\n",
    "# -Vocabulary: (formal vs. informal, technical vs. simple)\n",
    "\n",
    "# -Sentence Structure: (short and direct vs. flowing and complex)\n",
    "\n",
    "# Literary References: Consider if the rewritten style echoes a specific author or literary period (e.g., Shakespearean, Hemingway-esque).\n",
    "\n",
    "\n",
    "# **Original Text**:\n",
    "\n",
    "# {}\n",
    "\n",
    "\n",
    "# **Rewritten Text**\n",
    "\n",
    "# {}\"\"\"\n",
    "\n",
    "    inst = \"\"\"You'll be given an original text and a rewritten text generated by an LLM. \n",
    "\n",
    "Analyze the changes in the rewritten version and infer the likely prompt that led to those changes. \n",
    "\n",
    "Provide a detailed explanation of how you arrived at your inference step by step.\n",
    "\n",
    "\n",
    "**Original Text**:\n",
    "\n",
    "{}\n",
    "\n",
    "\n",
    "**Rewritten Text**\n",
    "\n",
    "{}\n",
    "\n",
    "\n",
    "You should response in the following format:\n",
    "\n",
    "**Inferred Promp**: ...\n",
    "\n",
    "\n",
    "**Chain of Thoughts**: ...\"\"\"\n",
    "\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "    # while True:\n",
    "\n",
    "    #     idx = random.randint(0, len(train_data) - 1)\n",
    "\n",
    "    #     if train_data[\"original_text\"][idx] != examples[\"original_text\"] or train_data[\"rewrite_prompt\"][idx] != examples[\"rewrite_prompt\"]:\n",
    "\n",
    "    #         break\n",
    "\n",
    "    # e_o_t = truncate_txt(train_data[\"original_text\"][idx],100)\n",
    "\n",
    "    # e_r_t = truncate_txt(train_data[\"rewritten_text\"][idx], 100)\n",
    "\n",
    "    # e_ans = train_data[\"rewrite_prompt\"][idx]\n",
    "\n",
    "    ot = truncate_txt(examples[\"original_text\"], 400)\n",
    "\n",
    "    rt = truncate_txt(examples[\"rewritten_text\"], 400)\n",
    "\n",
    "\n",
    "    # inst = inst.format(e_o_t, e_r_t, e_ans, ot, rt)\n",
    "\n",
    "    inst = inst.format(ot, rt)\n",
    "\n",
    "    rewrite_prompts = examples[\"cot\"]\n",
    "\n",
    "    text = USER_CHAT_TEMPLATE.format(inst, rewrite_prompts)\n",
    "\n",
    "    text += EOS_TOKEN\n",
    "\n",
    "    return {\"text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266fa7d-8a8b-482e-be84-9d7bcb02d31e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = train_data.map(formatting_prompts_func)\n",
    "test_data = test_data.map(formatting_prompts_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe933449-5160-4c5e-bd18-4a17724880ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(test_data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d332ee0-7dbb-4c5a-bb45-f581b9d39915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# sentence_model = SentenceTransformer('./sentence-t5-base').cpu()\n",
    "# from numpy import dot\n",
    "# from numpy.linalg import norm\n",
    "# def computeSharpenedCosineSimilarity(sent1, sent2):\n",
    "#     embeddings = sentence_model.encode([sent1, sent2])\n",
    "#     cos_sim = dot(embeddings[0], embeddings[1]) / (norm(embeddings[0]) * norm(embeddings[1]))\n",
    "#     return cos_sim ** 3\n",
    "\n",
    "# def compute_metrics(p):\n",
    "#     preds, labels = p\n",
    "\n",
    "#     # Remove ignored index (special tokens)\n",
    "#     preds = [\n",
    "#         [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(preds, labels)\n",
    "#     ]\n",
    "#     true_labels = [\n",
    "#         [l for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(preds, labels)\n",
    "#     ]\n",
    "#     print(decoded_preds)\n",
    "#     print(decoded_labels)\n",
    "#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True, device=\"cpu\")\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True, device=\"cpu\")\n",
    "\n",
    "#     res = 0\n",
    "#     for s1, s2 in zip(decoded_preds, decoded_labels):\n",
    "#         res += computeSharpenedCosineSimilarity(s1, s2)\n",
    "#     res /= len(decoded_preds)\n",
    "\n",
    "#     results = {\n",
    "#         'SCS': res\n",
    "#     }\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f2c781-3aae-47b2-9335-c684598eacc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import TrainingArguments\n",
    "response_template = \"<start_of_turn>model\\n\"\n",
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=0.05,\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=3407,\n",
    "    output_dir=\"./outputs/gemini_data/v7_cot_5k_remove_prompt\",\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    # neftune_noise_alpha=5,\n",
    "    args=args,\n",
    "    # compute_metrics = compute_metrics,\n",
    "    # data_collator=DataCollatorForCompletionOnlyLM(response_template=response_template,tokenizer=tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee1b7bb-733e-4cd0-8eb1-c43088dadaa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa42fd9-b5d3-489f-b0ca-6e75a5499209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c45d0-5b7d-4c78-bcc3-a68d526aea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(train_data, \"tran_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc34ce-8a0f-49b3-bfdd-4fd6b4a69a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save(test_data, \"test_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a02cad3-26da-456a-8029-39fef5e00736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.model.save_pretrained(\"./output/test_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e9af4-f844-47b5-a908-0d8e0bf3a302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# # Merge the model with LoRA weights\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"./gemma/gemma-7b-it-bnb-4bit/\",\n",
    "#     load_in_4bit = True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map={\"\": 0},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9a054-37f5-42cb-99ea-ff38a257ad10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from peft import LoraConfig, PeftModel\n",
    "# merged_model= PeftModel.from_pretrained(base_model, './output/test_lora_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e021201-2062-4654-9f6b-4d2e95031114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merged_model= merged_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a7dee-296e-47ba-9f19-5a2fc7314a32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merged_model.save_pretrained(\"./output/merged_model\",safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb053240-66f1-4974-ba89-577b15f9ed22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"./gemma/gemma-7b-it-bnb-4bit/\")\n",
    "# tokenizer.save_pretrained(\"./output/merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9d8c4-5421-4695-a401-96c1b8f45428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data[0]['text'].split('<start_of_turn>model\\n')[\n",
    "    0] + '<start_of_turn>model\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d373406-1ce2-4a8d-9c1f-5bb6613974fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        test_data[12]['text'].split('<start_of_turn>model\\n')[\n",
    "            0] + '<start_of_turn>model\\n'\n",
    "    ], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecf5daa-1a5f-4f87-a66c-184fb685858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[12]['rewrite_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9992b1-8726-4c79-8de0-5e94925edd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489a840-4a58-4e09-84b2-c13f8ab65ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "df = pl.read_csv(\"./datasets/train.csv\")\n",
    "df['original_text'][0]\n",
    "df['rewritten_text'][0]\n",
    "df['rewrite_prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e79d7a-1d24-4edf-96f6-fe0a0bc3d2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7466d-b350-4a58-b26d-d232db9e6a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Merge the model with LoRA weights\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "\n",
    "    load_in_4bit=True,\n",
    "\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "\n",
    "    \"./gemma/7b-it/\",\n",
    "\n",
    "    quantization_config=bnb_config,\n",
    "\n",
    "    device_map=\"auto\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee998c9a-ad39-4a80-acf4-b4efec75c113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "res_dict = {}\n",
    "\n",
    "files = os.listdir('./outputs/gemini_data/')\n",
    "\n",
    "test_data = torch.load('./test_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe6951-5697-4d28-af82-2f369fb6fae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from numpy import dot\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./gemma/7b-it/\")\n",
    "\n",
    "sentence_model = SentenceTransformer('./sentence-t5-base')\n",
    "\n",
    "\n",
    "\n",
    "def computeSharpenedCosineSimilarity(sent1, sent2):\n",
    "\n",
    "    embeddings = sentence_model.encode([sent1, sent2])\n",
    "\n",
    "    cos_sim = dot(embeddings[0], embeddings[1]) / \\\n",
    "        (norm(embeddings[0]) * norm(embeddings[1]))\n",
    "\n",
    "    return cos_sim ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3124360-29d0-4f44-af62-a699046c39ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5844b-64b1-40e6-8e93-0fcfb3f7c5b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for f in tqdm(files):\n",
    "    if not f.startswith('checkpoint'):\n",
    "        continue\n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        f\"./outputs/gemini_data/{f}/\")\n",
    "    res = []\n",
    "    truths = []\n",
    "    for example in test_data:\n",
    "        truths.append(example['rewrite_prompt'])\n",
    "        prompt = example['text'].split('<start_of_turn>model\\n')[\n",
    "            0] + '<start_of_turn>model\\n'\n",
    "        prompt_tokenized = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output_tokenized = lora_model.generate(**prompt_tokenized,\n",
    "                                               max_new_tokens=50,\n",
    "                                               use_cache=True,\n",
    "                                               do_sample=True,\n",
    "                                               temperature=0.8,\n",
    "                                               )[0]\n",
    "        # remove prompt from output\n",
    "        output_tokenized = output_tokenized[len(\n",
    "            prompt_tokenized[\"input_ids\"][0]):]\n",
    "        res.append(tokenizer.decode(\n",
    "            output_tokenized).split('<end_of_turn>')[0])\n",
    "    acc = 0\n",
    "    for s1, s2 in zip(res, truths):\n",
    "        acc += computeSharpenedCosineSimilarity(s1, s2)\n",
    "    res_dict[f] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38bb84-b752-4db9-bedd-c3fd1723fb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in res_dict:\n",
    "    res_dict[k] = res_dict[k] / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a9c96-bbeb-41c9-a3ce-d3975f6638f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(res_dict, \"res_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeb124e-1e9b-41a9-bf47-81e312895263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_list = [[k, v] for k, v in res_dict.items()]\n",
    "res_list.sort(key=lambda x: x[1], reverse=True)\n",
    "res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f190ef-e588-4b84-8451-6e1b4ddd4145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for s1, s2 in zip(res, truths):\n",
    "    acc += computeSharpenedCosineSimilarity(s1, s2)\n",
    "print(acc / len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d925f-9f47-4f58-b223-7b0358f7adb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604b258-d6a2-465b-a9ae-2675f3bb8206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# USER_CHAT_TEMPLATE = \"\"\"<start_of_turn>user\\nYou will be given original text and rewritten text.\n",
    "# Analyzing the changes in style, theme, etc., your task is to recovery the prompt which LLM used to convert the original text.\n",
    "# **Original Text**:\n",
    "# {}\n",
    "# **Rewritten Text**\n",
    "# {}<end_of_turn>\\n<start_of_turn>model\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262263b-cba6-4fde-a765-12de93b81bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USER_CHAT_TEMPLATE = \"\"\"<start_of_turn>user\n",
    "Original Essay:\n",
    "\\\"\"\"{}\\\"\"\"\n",
    "\n",
    "Rewritten Essay:\n",
    "\\\"\"\"{}\\\"\"\"\n",
    "\n",
    "Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\n",
    "You are trying to understand how the original essay was transformed into a new version.\n",
    "Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\n",
    "Start directly with the prompt, that's all I need. Output should be only line ONLY.<end_of_turn>\\n<start_of_turn>model\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b1d4e-a69d-40cb-86c1-59544553412a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./datasets/gemma100.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3331d780-771f-43cd-9ba6-871503709e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from numpy import dot\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer('./sentence-t5-base')\n",
    "\n",
    "\n",
    "\n",
    "def computeSharpenedCosineSimilarity(sent1, sent2):\n",
    "\n",
    "    embeddings = sentence_model.encode([sent1, sent2])\n",
    "\n",
    "    cos_sim = dot(embeddings[0], embeddings[1]) / \\\n",
    "        (norm(embeddings[0]) * norm(embeddings[1]))\n",
    "\n",
    "    return cos_sim ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d3c98-5e2e-462e-8abc-228796b1e343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./outputs/gemini_data/v1/checkpoint-1200/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7545f05f-e531-44ce-9b52-5a19705c958b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "ots = list(df['original_text'])\n",
    "rts = list(df['rewritten_text'])\n",
    "prompts_all = []\n",
    "for idx, (ot, rt) in enumerate(zip(ots, rts)):\n",
    "    prompts_all.append(USER_CHAT_TEMPLATE.format(ot[:1500], rt[:1500]))\n",
    "res = []\n",
    "for prompt in tqdm(prompts_all):\n",
    "    prompt_tokenized = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output_tokenized = lora_model.generate(**prompt_tokenized,\n",
    "                                           max_new_tokens=100,\n",
    "                                           use_cache=True,\n",
    "                                           do_sample=False,\n",
    "                                           temperature=0.8,\n",
    "                                           )[0]\n",
    "\n",
    "    # remove prompt from output\n",
    "    output_tokenized = output_tokenized[len(prompt_tokenized[\"input_ids\"][0]):]\n",
    "    res.append(tokenizer.decode(output_tokenized).split('<end_of_turn>')[0])\n",
    "acc = 0\n",
    "for s1, s2 in zip(res, list(df[\"rewrite_prompt\"])):\n",
    "    acc += computeSharpenedCosineSimilarity(s1, s2)\n",
    "print(acc / len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5769835-0c19-42df-be6e-574be1b9e43f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = pd.read_csv('./gemini_prompts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62ca26-bb91-4b70-b10c-d8e979c836d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ps = set(prompts['rewrite_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de722e9-835f-4383-9eb1-8d7d43b77c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd005d4-4e05-4d50-be0e-6ba7de2aa098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for p in ps:\n",
    "    if \"news article\" in p:\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0b6e2-b638-4b18-9d97-d9d0bddba0a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed1c64-6ee7-4a59-930a-214050bf589f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3fdcd3-4ca2-4929-a3c8-849190e87517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    [\n",
    "        \"<start_of_turn>user\\nYou will be given original text and rewritten text. \\nAnalyzing the changes in style, theme, etc., your task is to recovery the prompt which LLM used to convert the original text.\\n**Original Text**:\\nCTVNews.ca Staff She may not be strategizing offensive plays or coaching players on their tackles, but Catherine Raiche is an integral part of the team. The 28-year-old former corporate and tax lawyer has spent the past six months working as the Assistant General Manager for the Montreal Alouettes. She is also the only woman in football operations for the Canadian Football League. As part of her new position, Raiche handles players’ contracts, manages the salary cap and helps out with scouting new talent. “She is a task master,” the team’s General Manager Kavis Reed told CTV News. “You give her\\n**Rewritten Text**\\n## The Catherine Raiche Collection: Breaking Barriers in Football Operations\\n\\nCatherine Raiche is a force to be reckoned with, shattering glass ceilings and leaving her mark on the world of professional football. Her journey from corporate lawyer to assistant general manager for the Montreal Alouettes is a testament to her unwavering dedication and fierce determination.\\n\\nRaiche's expertise lies beyond simply managing contracts and budgets. She possesses a profound understanding of player management, talent acquisition, and operations. Her meticulous attention to detail ensures that players are well taken care of both on and off the field, fostering a positive and productive environment where athletes can thrive.\\n\\nThis collection embodies Ra\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    ], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "outputs = lora_model.generate(**inputs, max_new_tokens=50, use_cache=True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6145d12-71ac-429c-b1c7-c3a42414169f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab44ac-bf1a-450f-9401-52ae33ca2e92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_model = lora_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7652f47-7a79-489c-9c2a-16bd81ff8b20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = merged_model.generate(**inputs, max_new_tokens=50, use_cache=True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccba02-816e-4759-b09d-574feae97bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea401cc0-03d1-499e-83d2-434ceb1b940c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b096798-2f8c-4b5f-8783-a835d4edcd5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    [\n",
    "        test_data[0]['text'].split('<start_of_turn>model\\n')[\n",
    "            0] + '<start_of_turn>model\\n'\n",
    "    ], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = base_model.generate(**inputs, max_new_tokens=100, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e19224d-3c1f-4b15-b738-10d5747dd083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
